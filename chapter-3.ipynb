{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-13: What’s the difference between content-based recommender systems and collaborative filtering recommender systems? When would you use one over the other?\n",
    "\n",
    "### Example answer\n",
    "- Content-based recommender systems require knowledge of categorization or traits of the products being recommended to determine product similarity. Collaborative filtering relies on user behavior and user preferences to recommend products that users with similar tastes enjoyed and thus can be more ignorant about the products themselves.\n",
    "\n",
    "- Hence, content-based recommenders work well when there are not many users or items to construct the user-item matrix for collaborative filtering. In other words, content-based recommenders can still make recommendations when there is the “cold-start” problem, as long as there is information on the features of the items/products and some information on the traits or preferences of the users, without requiring more data on user behavior and interactions like collaborative filtering does.\n",
    "\n",
    "-On the other hand, collaborative filtering is suited for scenarios in which a lot of data on user behavior is available. At times, it can be hard to gather sufficient, meaningful features that describe the products, which makes content-based recommender systems ineffective. In these cases, collaborative filtering can be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anecdotally, I once worked on a project where a collaborative filtering algorithm (ALS) worked better for users who had used the web platform for a while but poorly for new users. Using content-based filtering with XGBoost worked better for new users, and we deployed different models depending on what type of user they were. Of course, this is only one example, and it may differ for your case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-14: What are some common problems encountered in recommender systems, and how would you resolve them?\n",
    "\n",
    "### Example answer\n",
    "- The cold-start problem: this is when there aren’t a lot of past data points available for an ML model to train on. Therefore, the model won’t be able to learn enough patterns from the past to accurately predict the correct results for new data points. In recommender systems, content-based systems can be used, which require less user-behavior data but do still require sufficient product-feature data. This can help with the cold-start problem and still provide recommendations to newer website users.\n",
    "- Recommender systems can also encounter challenges with data quality, a problem that isn’t exclusive to recommender systems. This can include errors in the source data—for example, due to a bug while ingesting the data. This issue can be addressed by analyzing where the source data has issues and then fixing it with the teams that handle data quality (at times, data engineers or platform engineers, or the MLEs and data scientists themselves). However, identifying that there is a data quality issue is important in the first place, and some preventative measures include using data quality monitoring tools like Great Expectations to alert the team when there are shifts in the data distribution or many missing values, for example.\n",
    "- When there are many missing values in an ML dataset, this is called sparsity. For example, users who sign up for a web platform with a questionnaire that asks for user preferences might not input several signup fields correctly or might skip them altogether. As an example, when someone signs up for a new Reddit account, there is a prompt that shows them common subreddits (subforums) that they may be interested in, but the user can skip this step. This is by design to make the web signup as frictionless as possible, but scenarios like this could cause data sparsity when you are trying to build a feature set for a RecSys. Possible solutions include imputation (e.g., filling in missing values with the mean or using a tree-based method to fill in the data), using collaborative filtering or matrix factorization techniques, feature engineering, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-15: What is the difference between explicit and implicit feedback in recommender systems? What are the trade-offs with using each type, respectively?\n",
    "\n",
    "### Example answer\n",
    "- Explicit feedback includes user ratings or reviews while implicit feedback has to be derived from available user behavior, such as the time spent on a web page or clickstream behavior. The benefits of explicit feedback include a clearly quantified rating to use in machine learning as well as clarity when compared to implicit feedback. However, explicit feedback can be harder to gather since not all users will leave a review after every interaction (most don’t).\n",
    "\n",
    "- Thus, measuring the user’s engagement or enjoyment via implicit feedback, such as video watch time or time spent reading on a website, might be used. Of course, this can lead to imperfect measures: is the user spending a long time on the webpage because they enjoyed the content or because they were confused about the text on it? Overall, it is important to consider the trade-offs, but in practice, you can often combine both feedback signals in your ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-16: How would you address imbalanced data in recommender systems?\n",
    "\n",
    "### Example answer\n",
    "- This is a common problem facing ML scenarios: there are a few classes or categories that have many more observations or data points than others, and there are many classes/categories that have so few observations that they form a long tail.41\n",
    "\n",
    "- To handle this issue, oversampling techniques may be helpful in simpler cases, such as creating more data points of categories that have fewer observations. However, when there are many classes/categories of observations, simple oversampling techniques won’t be able to alleviate this issue. Additional techniques such as feature engineering and ensemble methods can be used instead, or in conjunction with oversampling. An example of ensemble methods could be creating a separate recommender for popular items versus low-engagement items.\n",
    "\n",
    "- In companies like Amazon and Spotify, combining RecSys with other families such as reinforcement learning helps ensure that long-tail products, artists, or items are shown to users at least some of the time.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-17: Explain the DQN (deep Q-network) algorithm in reinforcement learning.\n",
    "\n",
    "### Example answer\n",
    "- DQN is an extension of Q-learning; DQN uses neural networks to approximate the Q-values, which represent the expected future reward from taking an action in a given state (the same definition as Q-learning). In DQN, you have two networks: a target network and the Q-network. The target network is responsible for predicting the best Q-value out of all actions that can be taken from a given state (the target Q-value). The Q-network takes the current state and action and predicts the Q-value for a particular action (the predicted Q-value). To improve the Q-network, the difference between the predicted Q-value, the target Q-value, and the observed reward is used as a loss function for the Q-network.\n",
    "\n",
    "- The weights of the neural network are updated based on the difference between the predicted Q-learning and the actual Q-value obtained through experience. The reason for using the target network is to ensure that the training results are more stable since with reinforcement learning and updating the Q-network with each step, the variance of each action can be quite large. After sufficient steps, the target network is updated with the new weights of the Q-network, and training continues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-18: As a follow-up question, could you explain the main modifications that DQN added on top of regular Q-learning?\n",
    "\n",
    "- One of the main new advances that DQN uses is experience replay. Experience replay is a component before the Q-network and target networks that uses a simple epsilon-greedy method to take an action in the current state in the real environment and get a reward. It saves these actions, states, and rewards as experiences for the networks to use as training data. The reason for using experience replay is the sequential nature of reinforcement learning; the training dataset for the networks should have the sequence of rewards and new states that result from each action in the previous state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-19: Explain exploration and exploitation in reinforcement learning with an example. What are the trade-offs of these two concepts? What are some ways you would balance exploration and exploitation?\n",
    "- I’ll use the example of an RL agent that is part of a simple self-driving car. During its exploration, it first finds that turning right at a red light isn’t penalized (going by rules in North America). The agent may continue to exploit this knowledge instead of trying something new and learning to stop at the red light, causing undesired behavior: never stopping at a red light. Therefore, it is important to encourage exploration as well so that the agent tries out new behaviors. As the agent has explored more iterations of the environment, then it is safer to continue to increase the exploitation parameter because at that point it may be more important for the agent to perform well and become an accurate self-driving car with the experience it has gathered so far. By allowing more exploration early on and then reducing exploration and increasing exploitation later, via techniques such as the epsilon-greedy policy, I can balance exploration and exploitation.\n",
    "- In summary, to balance exploration and exploitation, I’d use an epsilon-greedy policy55 so that the RL agent can explore more of the environment. As the agent interacts and learns more from the environment, the epsilon value is reduced, which makes the agent start to increase exploitation. Eventually, once the agent has explored sufficiently, it can exploit the good decisions it’s seen in the past and reduce exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-20: In the following scenario, you’ve found that the reinforcement learning algorithm keeps recommending an item that is incorrectly labeled as 10% of its sale price. What might have caused this, and what would you investigate, assuming that the data is all correct?\n",
    "\n",
    "- In the case that there is a reward function in our RL agent, I would investigate the reward/reward function and see if it’s rewarding perverse behavior of the RL agent. It could be that the agent is exploiting ways to increase users’ click-through rate artificially, for example, by recommending items that are highly discounted. The artificial increase in click-through rate in this case leads to a positive reward for the RL agent. If the reward takes into account the cost of the discounts in the reward function, then the agent will be less likely to only optimize click-through rate at the cost of losing money on the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-22: What are some common techniques of preprocessing in image-recognition tasks?\n",
    "- Common preprocessing techniques in image-recognition tasks include data normalization, data augmentation, and image standardization. Data normalization converts the numerical representation of the pixels in an image to a predefined range—for example, (0, 1) or (-1, 1). This is so the algorithms being applied to different layers can follow the same range. Data augmentation can help reduce overfitting on the training dataset. For example, if the training data coincidentally only includes cats that are facing to the right, then the CNN might not learn that cats facing to the left are also cats. Using various data augmentation techniques such as flipping, rotating, cropping, and so on, we can add more representations of the same object into the dataset and help the CNN learn to generalize the detection of objects. Image standardization makes the dataset easier to work with by ensuring that images have heights and widths that are close to one another. During this preprocessing step, images are resized so that they are within a certain range of widths and heights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-23: How might you handle class imbalance in image-recognition tasks?\n",
    "- There are several methods we can use to handle class imbalance in image-recognition tasks. A “class” in this case means a category or label; for example, an image may contain a “cat” or a “dog”. One way to handle class imbalance is to merge very similar categories, such as “orange” and “tangerine”. Of course, we’d have to determine the trade-off between the labels that are being merged. If the image-recognition model is responsible for labeling citrus fruits, then we should try another approach or merge other types of labels.\n",
    "- A second option is resampling, which generates synthetic data or duplicates data points in the minority class. There are built-in tools in TensorFlow and PyTorch to do this for image-recognition tasks. Another method to handle class imbalance is to adjust the CNN’s loss function to weigh mistakes on minority or rare categories more than mistakes on common classes and labels. This is to help avoid underfitting on rare categories due to class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-24: How would you handle overfitting in image-recognition tasks?\n",
    "- Adding a dropout layer (a type of regularization) within the CNN will set random neurons’ activations to 0; this prevents the layer from overexploiting a certain feature. Another method is early stopping, where training stops when there isn’t meaningful reduction of the loss (which the CNN is aiming to minimize). Reducing layer complexity can also reduce overfitting because when the CNN layers are too complex, it may find more patterns in the images than are meaningful. For example, many images of singers involve them standing on a stage and holding a microphone, and the model might learn that the presence of a microphone in the image means the presence of a singer. Another technique for handling overfitting in image recognition is data augmentation, which can help add more diversity into the training dataset and reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview question 3-25: How would you improve and optimize the architecture for a CNN used for image recognition?\n",
    "- If the existing network isn’t working well, for example, if it is underfitting and not classifying objects well, I might consider adding more layers of specific types—for example, adding a convolutional layer or rearranging the order of various layers. This is also how researchers optimize various algorithm architectures, such as creating variations of layers on ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
